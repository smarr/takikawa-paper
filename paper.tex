\documentclass[sigplan,screen]{acmart}

\setcopyright{acmlicensed}
\acmPrice{15.00}
\acmDOI{10.1145/3358504.3361232}
\acmYear{2019}
\copyrightyear{2019}
\acmISBN{978-1-4503-6987-9/19/10}
\acmConference[VMIL '19]{Proceedings of the 11th ACM SIGPLAN International Workshop on Virtual Machines and Intermediate Languages}{October 22, 2019}{Athens, Greece}
\acmBooktitle{Proceedings of the 11th ACM SIGPLAN International Workshop on Virtual Machines and Intermediate Languages (VMIL '19), October 22, 2019, Athens, Greece}

\def\AWFY{Are\,We\,Fast\,Yet\xspace}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\ie}{i.e.\xspace}
\newcommand{\eg}{e.g.\xspace}

\newcommand{\RR}[1]{{\color{red}RR: #1}}

\usepackage{listings}
\usepackage{xspace}

\usepackage{collab}

\collabAuthor{rr}{green!60!black}{Richard}
\collabAuthor{sm}{red}{Stefan}
\collabAuthor{kjx}{orange}{James}
\collabAuthor{isaac}{blue}{Isaac}
\collabAuthor{mwh}{purple}{Michael}


\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{codered}{rgb}{0.82,0.15,0.23}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

 
\lstdefinestyle{mystyle}{
    language=SAS,
    breakatwhitespace=true,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2,
    aboveskip=20pt,
    belowskip=10pt,
    xleftmargin=0.5cm,
    basicstyle=\footnotesize\ttfamily,
    commentstyle=\itshape\color{codegray},
    numberstyle=\footnotesize\color{codegray},
    stringstyle=\color{codegreen},
    keywordstyle = {\color{codepurple}},
    keywordstyle = [2]{\color{codered}},
    keywordstyle = [3]{\color{codered}},
    keywordstyle = [4]{\color{codegreen}},
    keywords={method,object,interface,type,var,def,return,class,for,in,if},
    otherkeywords = {:,->},
    morekeywords = [2]{:},
    morekeywords = [3]{->},
    morekeywords = [4]{true,false},
    morestring=*[d]{"},
    backgroundcolor={}
}

\lstset{style=mystyle}

\input{graphs/preamble}
\setcopyright{none}
\bibliographystyle{ACM-Reference-Format}

\begin{document}
\title{Which of my Transient Type Checks are not (Almost) Free?}

\author[Isaac O. G.]{Isaac Oscar Gariano}
\affiliation{
  \department{Engineering and Computer Science} % \department is recommended
  \institution{Victoria University of Wellington}
  \country{New Zealand}
}
\email{Isaac@ecs.vuw.ac.nz} % \email is recommended

\author[R. Roberts]{Richard Roberts}
\affiliation{
  \department{Computational Media Innovation Centre} % \department is recommended
  \institution{Victoria University of Wellington}
  \country{New Zealand}
}
\email{rykardo.r@gmail.com} % \email is recommended


\author[S. Marr]{Stefan Marr}
\orcid{0000-0001-9059-5180}
\affiliation{
  \department{School of Computing} % \department is recommended
  \institution{University of Kent}
  \country{United Kingdom}
}
\email{s.marr@kent.ac.uk} % \email is recommended


\author[M. Homer]{Michael Homer}
\affiliation{
  \department{Engineering and Computer Science} % \department is recommended
  \institution{Victoria University of Wellington}
  \country{New Zealand}
}
\email{mwh@ecs.vuw.ac.nz} % \email is recommended



\author[J. Noble]{James Noble}
\orcid{0000-0001-9036-5692}             %% \orcid is optional
\affiliation{
  \department{Engineering and Computer Science} % \department is recommended
  \institution{Victoria University of Wellington}
  \country{New Zealand}
}
\email{kjx@ecs.vuw.ac.nz} % \email is recommended




\begin{abstract}
One form of type checking used in gradually typed language is
\emph{transient type checking}: whenever an object `flows' through
code with a type annotation, the object is dynamically checked to
ensure it has the methods required by the annotation.

Although na\"ive implementations of transient type checks have a high run-time overhead, just-in-time compilation and optimisation in virtual machines can eliminate much of this overhead.  Unfortunately
the improvement is not uniform: while most type checks can be optimised
away, some will significantly decrease a program's
performance, and some may even increase it.  

In this paper, we refine the so called ``Takikawa'' protocol, and use it to identify which type annotations have the greatest effects on performance. In particular, we show how graphing the performance of such benchmarks when varying which type annotations are present in the source code can be used to discern potential patterns in performance. We demonstrate our approach by testing the Moth virtual machine: for many of the benchmarks where Moth's transient type checking impacts performance, we have been able to identify one or two specific type annotations that are the likely cause. Without these type annotations, the performance impact of transient type checking becomes negligible.

Using our technique programmers can optimise programs by removing expensive type checks, and VM engineers can identify new opportunities for compiler optimisation.
\end{abstract}


%% Generate at 'http://dl.acm.org/ccs/ccs.cfm'.
\ccsdesc[500]{Software and its engineering~Software performance}
\ccsdesc[300]{Software and its engineering~Object oriented languages}
\ccsdesc[300]{Software and its engineering~Just-in-time compilers}

\keywords{dynamic type checking, gradual types, optional types, Grace,
performance evaluation, benchmarking, object-oriented programming}
\maketitle


\section{Introduction}
Gradual typing aims to add static type annotations to dynamic languages, increasing their safety while maintaining flexibility \citep{GiladPluggable2004,Siek2006,XXXSiek2015}, and/or, permitting dynamic type annotations within static languages, increasing flexibility whilst maintaining some safety \citep{AbadiTOPLAS1991}.

There is a vast spectrum of different approaches to gradual typing \cite{kafka18,bensurvey18icfp}. Here we measure the performance of ``transient'' or ``type-tag'' checks (as in Reticulated Python), which offer first-order semantics: they check that an object's type constructor or names of supported methods match any static types that the object flows through, but not the return/argument types of such methods \cite{Siek2007,Bloom2009,concrete15,reticPython2014,Greenman2018}.

Unfortunately most gradual systems with run-time semantics, as opposed to type erasure as in TypeScript, impose significant run-time performance overheads. This has lead to a significant body of research to develop techniques to optimise gradual typing \citep{Vitousek2017,Muehlboeck2017,Bauman2017,Richards2017,Greenman2018}.

This has also lead to a technique to evaluate the performance of gradual typing, such as the so called so-called ``Takikawa'' protocol \cite{Takikawa2016,Greenman2019jfp,vitousek-transient-arXive-2019}. The Takikawa protocol was created to measure the run-time cost of gradual typing by testing various configurations of typed/untyped code. This approach was designed to characterise how the amount of typed and untyped code influences performance.
In particular it shows that how simply adding more type annotations does not always produce a uniform effect on performance. Here we adapt their approach in order to identify individual type annotations that may be responsible for performance effects.

This paper builds upon our recent work on optimising transient type checks \cite{Roberts2017,roberts-and-co-ecoop-2019} to make the following contributions:
\begin{itemize}
  \item an approach to identifying individual gradual type annotations
        that cause significant performance effects
  % SM: after discussion with Isaac, this was done
  %     by some Reticulated Python paper before
  % and Isaac's adaptation is restricted to using rounding rules
  %    (>= 0.5 is round up) that give useful results (at least 1 type check)
  %    for selecting the number of type checks
  % \item adapting the Takikawa protocol for languages and implementations
  %       where performance effects are dominated by type annotations instead
  %       of transitions between typed and untyped modules
  \item an observation that the overhead of Moth's transient type checking on small benchmarks (with 10--250 type annotations) is usually caused by only one or two type annotations
\end{itemize}


The next section discusses dynamic type checks and gradual typing in
Moth (an implementation of the Grace language), then section~\ref{s-meth} describes our benchmarking protocol. Section~\ref{s-overall} then presents the overall results of
our benchmarks, while section~\ref{s-individual}
looks at the results of benchmarking individual type checks.
Section~\ref{s-rel} presents some additional related work, and finally \ref{s-concl} summarises our results, and briefly considers threats to validity.

\section{Background}
\label{s-bg}

Our work is based on the Moth virtual machine 
\cite{Roberts2017,roberts-and-co-ecoop-2019},
an implementation
of the Grace programming language 
\citep{graceOnward12,graceSigcse13}.
Moth is based on the Graal and Truffle toolchain
\cite{Wurthinger:2017:PPE,Wurthinger2013},
and developed from a Newspeak implementation based on the  Simple
Object Machine \cite{Daloze2016,SOMns}.

\subsection{Grace and Transient Type Checking}

Grace is an object-oriented, imperative, educational programming
language, with a focus on introductory programming
courses, but also intended for more advanced study and research \citep{graceOnward12,graceSigcse13}.
%
While Grace's syntax draws
from the so-called ``curly bracket'' traditions of C, Java, and
JavaScript, the structure of the language
is in many ways closer to Smalltalk:
all computation is done via dynamically dispatched  ``method requests''
where the object receiving the request decides what code to run,
control structures are built out of lambda expressions support ``non-local'' returns, i.e. they can return to the point where execution first encountered the lambda \citep{bluebook}.  In
other ways, Grace is closer to JavaScript than Smalltalk: Grace
objects are created from object literals, rather than by
instantiating classes \citep{Black2007-emeraldHOPL,JonesECOOP2016} and
objects and classes can be deeply nested within each 
other \citep{betabook}.

\paragraph{Grace's Typing}
In Grace, all declarations can be annotated with types.
As Grace is designed to support a variety of teaching methods, implementation of Grace are free to check such type annotations  statically, dynamically, or not at all.
The type system of Grace is intrinsically gradual:%
%
~type annotations should not affect the semantics of a correct
program \citep{XXXSiek2015}. The type system
includes a distinguished ``{Unknown}'' type which matches any other type; this unknown type is the default when type annotations are omitted.

Static typing for the core of Grace's type system has been described
elsewhere \citep{TimJonesThesis};
here we explain how
these types can be understood 
dynamically, from the Grace programmer's point of view.
Grace's types are structural \citep{graceOnward12},
that is, an object conforms to a type whenever it conforms to the ''structural`` requirements of a type,
rather than requiring classes or objects to explicitly declare their intended type.

In Grace, types specify a set of method signatures that an object must provide. A type expresses the requests an object can respond to, for example whether a particular accessor is available, rather than a location in a class hierarchy.

\paragraph{Moth's Transient Type Checking}
Moth's implementation of transient type checks are only only first-order.
Moth only checks dynamically that an object has methods of the same name and arity as are required by a type:  any argument and return types of such methods are not checked.

In particular, Moth performs the following type checks at run time:
\begin{itemize}
\item when a method is requested, arguments that are passed are checked against the corresponding parameter type annotations of the called method, this is done before the body of the method is executed;
\item when the body of a method has finished executing, but before it returns to its caller, the method's return value is checked against the return type annotation of the called method;
\item whenever a variable is read or written to, its value is checked against the type specified by the variables declaration.
\end{itemize}

To see how this works in practice, consider this piece of Grace code:

\begin{minipage}{\linewidth}
\begin{lstlisting}
def o = object {
   method three -> Number {3}
}
type ThreeString = interface {
   three -> String
}
def t : ThreeString = o
printNumber (t.three)
\end{lstlisting}
\end{minipage}

Moth will perform dynamic type checks:

\begin{itemize}

\item on line 7,
when the \code{o} object initialises the variable \code{t},
Moth checks that \code{o} has a 0-argument method called \code{three};

\item on line 8,
when the value of \code{t} is read,
Moth checks that its value (\code{o}) still has a \code{three} method;

\item on line 2,
when the method requested by ``\code{t.three}'' returns,
Moth checks that returned value conforms to the \code{Number} type;
and (presumably) within the definition of
%
\code{printNumber(n :   Number)}
%
(not shown), Moth will again check that the value is a \code{Number}.
\end{itemize}

Note that we never check
whether the result of requesting ``\code{t.three}'' is actually
a \code{String} (as one may expect from line 5) because Moth only performs first-order type checks
(it checks whether objects have conforming methods) not higher-order
checks (whether the argument and result types of methods' conform). In addition, Moth
only checks when values flow through explicit type annotations.
This is why the type declared in lines 4-6 is checked only on line 7
(where it is mentioned explicitly); and the check only requires the
presence of a method called \code{three}, regardless of the method's
declared return type.

\paragraph{Moth's Optimisation}
We are developing Moth as a
research platform \cite{roberts-and-co-ecoop-2019}. Like other VMs
based on the Truffle and Graal toolchain, Moth is a self-optimising
AST interpreter \cite{Wurthinger:2012:SelfOptAST}. 
The key idea is that an AST rewrites itself based on a program's run time values
to reflect the minimal set of operations needed to execute the program
correctly. The rewritten AST is then compiled into efficient machine
code. This rewriting often depends on the dynamic types of the
objects involved. In the simplest case, a ``self'' call (when one method
on an object requests a second method on the exact same object) will
always result in executing the exact same method. Thus the called method can be inlined into
the callee, avoiding overhead of a machine-level subroutine
invocation and an object-oriented dynamic dispatch.


Moth relies on a number of standard techniques for optimising
object-oriented programs.
``Shapes'' \citep{woss2014object} capture information about objects'
structures and (run time) 
field types, allowing a just-in-time compiler to
represent objects in memory similarly to C structs and, consequently,
can generate highly efficient code.
``Polymorphic inline caches''
\citep{Hoelzle:91:PIC} use object shapes to cache the results of
method lookups, avoiding expensive class hierarchy searches or
indirect jumps through virtual method tables. 
Since Moth is built on the Truffle framework,
Graal comes with  additional support for partial evaluation,
which enables efficient native code generation for
Truffle interpreters \citep{Wurthinger:2017:PPE}.
% SM: kjx says it's fine, so, drop for now
% \RR{This last sentence seems out of place, does it belong here?}
% \kjx{yes I think it's fine}

\section{Experimental Methodology}
\label{s-meth}

Our goal is to identify which type annotations in Grace programs
cause performance effects.
To this end, we built upon the so-called ``Takikawa'' or ``Takikawa-Greenman'' evaluation protocol \cite{Takikawa2016,Greenman2019jfp}.
It uses $2^N$ \emph{configurations} of each benchmark.
A configuration is a particular mix of static and dynamically typed code, forming a lattice of configurations.
We only test a relatively small sample of this lattice,
which is in our experience sufficient to pinpoint performance anomalies caused
by type annotations.

\subsection{The Takikawa Protocol}
The Takikawa evaluation protocol was originally proposed for Typed
Racket, where static vs dynamic typing is set per-module, so $N$ is
the number of modules. The original Takikawa protocol also suggested a sampling strategy, where $10N$ configurations are randomly chosen from the lattice, however the lattice is a binomial distribution, meaning the majority of chosen benchmarks will have around $N/2$ type annotations.

Grace allows programmers to choose for each individual declaration whether it should be type-checked, and thus follows languages such as
Reticulated Python \cite{reticPython2014,monotonic2015,Vitousek2017}.
This means in Grace $N$ is the number of type annotations in the program.
This makes it infeasible to checking an entire lattice, even for a moderately sized benchmark.
Vitousek et al.\ therefore modified the Takikawa protocol for
these kinds of languages by taking an approach based on sampling
\cite{vitousek-transient-arXive-2019}.  The Takikawa-Vitousek protocol
divides the number of type annotations in a fully-typed program into
a maximum of 100 intervals, and then randomly generates ten programs within
each interval by erasing type annotations.
However, this approach was designed for benchmarks with large numbers of type annotations, as well as for a larger sample size than our work.

 % Here we apply this methodology to our previous work \cite{roberts-and-co-ecoop-2019}, where we evaluated the performance of transient type checks in Moth by using a suite of benchmarks.

Unlike prior work, we wish to identify which type annotations cause anomalies,
% To investigate Moth's support for transient type checks further, 
and thus we adapted the Takikawa protocol
and took inspiration from the Takikawa-Vitousek variant.
For each benchmark, we generated 100 partially typed versions,
or fewer if the benchmark has less than 11 types.
We did an even split so that for each $i \ge 1$ and $i$ < $N$, we generated roughly the same number of configurations with $i$ type annotations. We used Robert Floyd's sampling algorithm \cite{Bentley:1987:PPS:30401.315746} to randomly choose the type annotations each configuration contained, and we ensured that no duplicate configurations were generated. In addition to these, we tested fully untyped and typed versions, for a total of 102 configurations per benchmark (or 97 in the case of our Storage benchmark, since it only has 10 type annotations).

\subsection{The Benchmarks}

For this work, we rely on the benchmark suite compiled for previous work
\cite{roberts-and-co-ecoop-2019}.
It is a collection of 21 benchmarks in total,
derived from the \AWFY
benchmark suite \cite{Marr2016} and other benchmarks
from the gradual-typing literature.

% In our previous work , we evaluated the performance of transient type checks using 21 benchmarks. We started with %the 14 benchmarks of the : Bounce, CD, DeltaBlue, GraphSearch, Havlak, Json, List, Mandelbrot, NBody, Permute, Richards, Sieve, Storage and Towers benchmarks. We added another \ugh{nine}\sm{14 plus 9 isn't 21} benchmarks taken from the gradual-typing literature.\sm{it's also unclear whether this is about previous work or what we do now.}
% Our goal was to complement the \AWFY with additional ones that are used for similar experiments and can be ported to Grace.
% We surveyed a number of papers
% \citep{Takikawa2016,Vitousek2017,Muehlboeck2017,Bauman2017,Richards2017,Stulova2016,Greenman2018}
% and selected additional benchmarks that have been used by multiple papers.
% Two of these benchmarks (NBody and Sieve) overlapped with the \AWFY suite,
% or were available in different versions.
% While not always behaviourally equivalent,
% we chose the \AWFY versions since we already used them to
% establish a performance baseline.
% The selected benchmarks as well as the papers in which they were used are shown in
% Table~\ref{tab:gradual-benchmarks}.
% \sm{this paragraph needs to be cleaned up.
% We need to say what we use now. Clearly, and not intermingled with previous work.
% And then, afterwards we can say whether this is the same we did before. or how it differs.
% }
%
% \begin{table}[htb]
%   \caption{Benchmarks selected from literature.}
%   \label{tab:gradual-benchmarks}
%   \begin{center}
%     \begin{tabular}{l l r}
%       Fannkuch & \cite{Vitousek2017,Greenman2018} \\
%       Float & \cite{Vitousek2017,Muehlboeck2017,Greenman2018} \\
%       Go & \cite{Vitousek2017,Muehlboeck2017,Greenman2018} \\
%       NBody & \cite{Kuhlenschmidt:2018:preprint,Vitousek2017,Greenman2018} & used \cite{Marr2016} \\
%       Queens & \cite{Vitousek2017,Muehlboeck2017,Greenman2018} & used \cite{Marr2016} \\
%       PyStone & \cite{Vitousek2017,Muehlboeck2017,Greenman2018} \\
%       Sieve & \cite{Takikawa2016,Muehlboeck2017,Bauman2017,Richards2017,Greenman2019jfp} & used \cite{Marr2016} \\
%       Snake & \cite{Takikawa2016,Muehlboeck2017,Bauman2017,Richards2017,Greenman2019jfp} \\
%       SpectralNorm & \cite{Vitousek2017,Muehlboeck2017,Greenman2018} \\
%     \end{tabular}
%   \end{center}
% \end{table}

Each benchmark has an almost complete set of type annotations,
\ie, static type annotations are present at almost all possible points.
%ISAAC: I changed this as it was misleading, in a few cases, some of the type annotations are in fact "Unknown", which is exactly what my "removing" of type annotations places

In our previous work, we \cite{roberts-and-co-ecoop-2019}
we determined that the overhead of type checking on Moth is on average
of 5\% (min.\ -13\%, max. 79\%).
This compares the peak performance of Moth with all
checks disabled against an execution that has all checks enabled. 
%  The
% results of this evaluation demonstrated that Moth's transient type checking has an overhead  compared to Moth's peak performance with type checking disabled.
%
% We did not measure programs that mixed typed and untyped code because
% with our implementation technique we expected fully typed programs to
% have the largest overhead, which is not the case for other kinds of
% gradual type checking.

\subsection{Experimental Set Up}
As in our previous work \cite{roberts-and-co-ecoop-2019} which used the same set of benchmarks, to account for the complex warmup behaviour of modern systems \citep{Barrett:2017:VMW} as well as
the non-determinism caused by \eg garbage collection and cache effects, we ran each benchmark for 1000 iterations in the same invocation of Moth, and discard the first 350 iterations to ignore warmup JIT compilation.

Though outliers remain visible in the plots for each individual benchmark, the largest 95\% confidence interval we obtained (over the mean time after warmup) for any of experiments was $\pm8.3\%$ (for the PyStone benchmark).

All our experiments used the same machine, Graal, and Moth as previously; the machine has two Intel Xeon E5-2620 v3 2.40GHz, with 6 cores each, for a total of 24 hyperthreads.
The machine was running Ubuntu Linux 16.04.6, with Kernel 4.4, and we used ReBench 1.0 \citep{ReBench:2018} and Java 1.8.0\_191 Graal 0.43. Benchmarks were executed one by one to avoid interference between them. The analysis of the results and plots where generated using Python 3.7.3 and PGFPLOTS 1.16. To enable reproductions, the scripts we used to generate and run our experiments, including the source code for all the configurations tested, are available online.\footnote{\url{https://gitlab.ecs.vuw.ac.nz/isaac/Moth-Takikawa}}

In our previous work we also compared the performance of untyped code on Moth against state-of-the-art VMs: Java, Node.js using the V8 JavaScript VM, and the Higgs JavaScript VM. Java was the fastest of these, and on average V8 was about 1.8x slower than Java, Moth was 2.3x slower, and Higgs was 10.4x slower.
Hence we continue to argue that ``Moth is a suitable platform to assess the impact of our approach to gradual type checking, because its performance is close enough to state-of-the-art VMs, and run-time overhead is not hidden by slow baseline performance.''

\section{Performance of Benchmark Configurations}
\label{s-overall}

\begin{figure*}
	\input{graphs/full-scatter}
	\caption{Graphs of (at most) 102 configurations in the typing lattices for each benchmark. Time is measured as the mean of the 351\textsuperscript{st} to the 1,000\textsuperscript{th} benchmark iteration under a single invocation of Moth (lower is better).}
	\label{f:full}
\end{figure*}

Before we start to investigate specific type annotations,
we present the performance measurements of our sample of the typing lattice configurations in figure~\ref{f:full}.
These results are the foundation for a more detailed analysis.

Following \cite{vitousek-transient-arXive-2019}, the points on each graph in figure~\ref{f:full} show the average execution of each individual configuration. The x-axis represents the proportion of type annotations for each configuration, with the left- and right-most points showing the times for the fully untyped and typed configurations respectively. The execution time in milliseconds is shown on the left y-axes, and time relative to the fully untyped configuration is shown on the right y-axes.

Most of the graphs are essentially horizontal lines, indicating that the overhead of including type annotations is negligible. The plots for CD and Richards show a roughly linear increase, i.e.\ for these two benchmarks, adding type annotations reduces performance linearly. On the other hand, the plots for Go, Permute, DeltaBlue, and Storage show \emph{decreases}: i.e. adding more type annotations \emph{improves} performance of these benchmarks.

By inspecting the scatterplots, we observe that the performance of
Moth in almost any configuration of a benchmark is bounded by
the performance in the untyped and fully-typed configuration.
That is, for these benchmarks on transient type checks,
measuring just the untyped and fully-typed configurations
would provide excellent estimates of a benchmark's
performance bounds.  This is different to the experience
of other kinds of gradual typing, where the best, and most
importantly worst, configurations are not always those
fully typed or fully untyped \cite{Greenman2019jfp}.
%
However, the Richards benchmark \textit{does} have
sections outside these bounds, and isolated executions of
a couple of others (Fannkuch and DeltaBlue) are also outliers.
%
We believe that the 0\% and 100\% bounds are nonetheless a
reasonable heuristic estimator in most cases, and we will
examine further the outlying benchmarks.


Of particular note is that some of the graphs (Permute, Storage, Towers, and Richards) show bimodal performance profiles, that is, two separate roughly-horizontal lines. Presumably Moth can remove all the overhead from some configurations, but in others there must be one or more type checks that cannot be optimised away. List shows three performance modes: the graph consists of three mostly flat lines, at about 1 times slower, 1.5 times slower, and 1.8 times slower than untyped code.



\section{Identifying Type Annotations With Signification Performance Impact}
\label{s-individual}
\begin{figure*}
	\input{graphs/pattern}
	\caption{Pairs of colour coded scatter and column graphs. The scatter graphs represent the performance of a sample of the typing lattices. The column graphs show the performance of every configuration with only one type annotation. The scatter plots and column graphs are colour coded based on whether a particular type annotation or two are present in the source code.}
	\label{f:pattern}		
\end{figure*}

We hypothesise that the previously identified bi/tri-modal performance behaviour
as seen in the graphs for Permute and others (cf. figure~\ref{f:full}) are caused by only a few type annotations: i.e.\ there are a very few annotations that determine each benchmark's performance.

% In order to identify those type annotations, we generated coloured
% scatter plots for each benchmark based on whether each individual type
% annotation was present in that configuration or not. The position of
% the dots in each coloured plot are the same as Figure~\ref{f:full}, but
% they are coloured as to whether each annotation is there or not: This resulted in 1,775 scatterplots --- one for each type annotation for each benchmark. We then manually inspected all these scatterplots, though almost all of these graphs showed no discernible pattern, we were able to identify some for about a third of the benchmarks.

To verify this hypothesis 
% As an alternative, less labour intensive approach to identifying
% potentially influential type annotations,
for each type annotation we measured one additional
configuration, with only that single type annotation present.
We did this to compare the overhead of
each type annotation in isolation against the no-typecheck baseline.

Figure \ref{f:pattern} shows the results of these experiments.
% , together with patterns we manually identified.
It shows a pair of graphs for a selection of ten benchmarks: it's associated typing lattice scatter plot and a column graph showing the results of these single type annotation experiments.

The column graphs, right of the corresponding scatter plot, shows the execution time of configurations with only a single type annotation, the x-axis indicates the index of this annotation, thus the first column represents the first annotation, and the last column represents the last one (in the order they appear in the source code). The y-axes for the column graphs are the same as the associated scatter graphs.

% SM: already said before
% The column graphs show the execution times of each configuration for each individual annotation in a benchmark. The x-axis of the graphs indicates the index of this annotation in the order they appear in the source code. The first column represents the first annotation, and the last column represents the last one. The y-axes for the column graphs are the same as the associated scatter graphs, showing the execution time of each configuration.
% \footnote{Although the right y-axes for the column graphs appear to be identical to the scatter plot axes, they are relative to performance results collected on different days: this was to ensure that the performance of the test computer had not changed significantly from when the typing lattice experiments were run.}

For each benchmark we highlighted one or two type annotations that seem to show a pattern for the typing lattice performance scatter plots, or in the case of SpectralNorm and Json had higher than usual columns. The identified types are those represented by the red and blue columns. The scatter plots are colour-coded accordingly: a red or blue circle represents configurations with the given type annotation present, but not both, purple circles represent configurations with both type annotations present, and grey circles represent those with neither. Though we exhaustively inspected such colour coded scatter plots for all 1,775 type annotations across all 21 benchmarks, the only patterns we noticed are those shown in \ref{f:pattern}.

As can be seen, most of the time the patterns we found in the typing lattice graphs correspond to outliers in the single type annotation column graphs. For the Snake, Towers and CD benchmarks, there is clearly a pattern where an individual type annotation (highlighted in red) appears responsible for the upper/slowest half of the typing lattice graphs. For Permute there is actually a significant performance \emph{increase}, indicated by the lower half of the lattice being highlighted in red. DeltaBlue and Go also show a slight increase in performance, but here this is caused by two type annotations (in red and blue); this effect is not cumulative, however: the purple dots are about the same height as the red and blue ones, so \emph{either} annotation is sufficient for the full benefit.

List is interesting as it demonstrates that the red and blue type annotations both cause a significant decrease in performance, which is even greater when both are present. Richards is particularly odd as it shows a performance decrease that occurs when the red type annotation is present, but not the blue one. We had previously identified this benchmark and the aforementioned type annotations \cite{roberts-and-co-ecoop-2019} as being the worst case for Moth.

For Json and PyStone, although we found outliers in the column graphs (such as types \#112 and \#16, highlighted in red), we were unable to find a matching pattern in the configuration performance.
Finally, for DeltaBlue there is a noticeable spike in the column graphs (at \#198), however we again found no apparent relation between this outlier and the overall performance.

In particular, observe that the grey dots for Snake, Towers, DeltaBlue, Go, and List are mostly flat horizontal lines, this indicates that by simply deleting the red and blue type annotations, the performance impact of transient type checking becomes negligible. However for CD and Richards, the transient type checking overhead of the grey dots is roughly linear, albeit still less than with the red and blue type annotations present. Thus we can observe that usually, only a couple of type annotations are responsible for the overhead caused by Moth's transient typed checking, whereas the rest are ``free''.

The remaining benchmarks, which we have not shown in figure \ref{f:pattern}, have even flatter column graphs than PyStone, and we could not identify any patterns in the typing lattices. Of those we did not show, the greatest difference in single-type configurations relative to the untyped configuration was $5.07\%$ (for the Float benchmark).
%For example for Snake, the dots covered in red (corresponding to configurations with type annotation number 61) are noticeably higher than the grey ones (those without type annotation 61), thus it appears that the 61\textsuperscript{st} type annotation causes a noticeable slowdown, which is also exemplified by the higher than average red column. The graphs for CD (type \#107), Towers (type \#7) show similar patterns. The graph for Permute is similar, but it shows that type annotation 8 actually makes the benchmarks \emph{faster}.
%In some cases, more than two type annotations have an effect, for DeltaBlue and Go it appears that types 154/155, and 31/47 (respectively) all cause an increase in performance, but this is not cumulative.
%However with List, the performance identified by type annotations 17 and 18 is actually comulative

%For most of the graphs, the identified type annotations are the same, however this is not always the case. For SpectralNorm and Json, although we found outliers in the column graphs (types 27 and 83, highlighted in red), there appears to be no relationship with the typing lattice.
%For DeltaBlue and Richards, type annotations 62 and 24 on the column graphs appear to be significant, however we did not see any kind of pattern their.

\section{Related Work}
\label{s-rel}

The high-performance computing community has been investigating how
tools and visualisations can help developers to utilise their systems
more efficiently \citep{Papenhausen:2016:IVT,daSilva:2019:PSV}.
Their focus is typically on parallelisation opportunities,
guided by run-time feedback, cost models, or heuristics. 
Their large body of work \citep{Isaacs:2014:PerfViz} uses various approaches,
though, we are not aware of work that has used an approach similar to ours.

At the moment, our approach to identifying type annotations that case
performance anomalies is not integrated into a development environment.
Though, for instance Optimization Coaching
\citep{St-Amour:2012:OCO} is a promising direction.
Optimization Coaching uses feedback from the runtime to guide developers
to insert or change type declarations 
to enable a compiler to generate a more optimal program.
In this spirit, we would eventually want to achieve the same, although
in our case,
we need to run full experiments to get the necessary information.


\section{Discussion and Conclusion}
\label{s-concl}

In this paper we have investigated how benchmarks can be
repurposed to determine precisely which transient type checks are
likely resistant to the optimisations provided by a just-in-time compiling
virtual machine.
However, detailed analysis will be needed in order to identify exactly what causes such performance effects, such as that we previously did for the List benchmark \cite{roberts-and-co-ecoop-2019}.
We observed that many of our benchmark results
conducted under the Takikawa protocol 
showed a characteristic bimodal performance profile: some
of the benchmark configurations ran significantly slower than
the remaining configurations. We also observed trimodal profiles, as well as performance increases when type annotations where added.

By inspecting graphs of the performance of where only one type annotation is present, we can easily identify type annotations that likely have a significant effect on performance. By then inspecting the typing lattice graphs colour coded based on whether such type annotation was present or not, we were often able to notice patterns.
In particular, these patterns suggested the just one or two type annotations are likely responsible for the bimodal performance and most of the overhead caused by Moth's transient type checking.
Though every type annotation that showed a significant effect across the typing lattice also showed a significant performance effect when no other typing annotations where present, the converse did not hold.

This is preliminary work, in particular we have not yet identified exactly why these type annotations show an effect on performance.
There are also a number of threats to validity. 
Regarding construct validity, 
our underlying
implementation may contain undetected bugs that affect the semantics
or performance of the gradual typing checks. Regarding internal
validity,
our benchmarking harness runs on the same implementation
and therefore is subject to the same issues.
Regarding external validity, 
%\RR{The next two sentences seem to be in the wrong place (they state an advantage rather than a threat); maybe move them into the previous paragraph?}
%\kjx{I'm happy with them here about external validity}
Moth is built
on the Truffle and Graal toolchain, so we expect
to resemble other Graal
VMs doing similar AST-based optimizations of transient type checks.
Because we rely on common techniques, 
we expect our results to be transferable to other JIT implementations as well.

Finally, it is not clear how our results would transfer
to other gradually typed-languages or other semantics for gradual typing.
Our benchmarks do not depend on any features of Grace
that are not common in other object-oriented
languages, but as Grace lacks a large corpus of programs the
benchmarks are necessarily small and artificial.
The advantage of Grace for this research is
that their relative simplicity means we have been able to build an
implementation that features competitive performance with significantly less
effort than would be required for larger and more complex languages.

In the future, we hope to investigate statistical techniques to
determine the significance of each type annotation's contribution to a
programs overall performance. We would also like to investigate 
whether this approach can assist with optimisations for programmers' day-to-day  development, or help VM engineers identifying performance bugs in the underlying virtual machines.

\begin{acks}
The authors would like to thank the anonymous reviewers for their
valuable comments and helpful suggestions. This work is supported
in part by the \grantsponsor{Marsden Fund}{Royal Society of New Zealand (Te Ap\={a}rangi) Marsden Fund (Te P\={u}tea Rangahau a Marsden)}{https://royalsociety.org.nz/what-we-do/funds-and-opportunities/marsden/} under grant \grantnum{Marsden Fund}{VUW1815}.
\end{acks}

\bibliography{references}
\end{document}
