\documentclass[sigplan,10pt,review,screen]{acmart}\settopmatter{printfolios=true}

\acmConference[VMIL'19]{ACM SIGPLAN International Workshop on Virtual Machines and Intermediate Languages}{October 22, 2019}{Athens, Greece}
\acmYear{2019}
\acmISBN{978-x-xxxx-xxxx-x/YY/MM}
\acmDOI{10.1145/nnnnnnn.nnnnnnn}
\startPage{1}


\def\AWFY{Are\,We\,Fast\,Yet\xspace}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\ie}{i.e.\xspace}
\newcommand{\eg}{e.g.\xspace}

\newcommand{\RR}[1]{{\color{red}RR: #1}}

\usepackage{listings}
\usepackage{xspace}

\usepackage{collab}

\collabAuthor{rr}{green!60!black}{Richard}
\collabAuthor{sm}{red}{Stefan}
\collabAuthor{kjx}{orange}{James}
\collabAuthor{mwh}{purple}{Michael}


\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{codered}{rgb}{0.82,0.15,0.23}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

 
\lstdefinestyle{mystyle}{
    language=SAS,
    breakatwhitespace=true,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2,
    aboveskip=20pt,
    belowskip=10pt,
    xleftmargin=0.5cm,
    basicstyle=\footnotesize\ttfamily,
    commentstyle=\itshape\color{codegray},
    numberstyle=\footnotesize\color{codegray},
    stringstyle=\color{codegreen},
    keywordstyle = {\color{codepurple}},
    keywordstyle = [2]{\color{codered}},
    keywordstyle = [3]{\color{codered}},
    keywordstyle = [4]{\color{codegreen}},
    keywords={method,object,interface,type,var,def,return,class,for,in,if},
    otherkeywords = {:,->},
    morekeywords = [2]{:},
    morekeywords = [3]{->},
    morekeywords = [4]{true,false},
    morestring=*[d]{"},
    backgroundcolor={}
}

\lstset{style=mystyle}

\input{graphs/preamble}
\setcopyright{none}
\bibliographystyle{ACM-Reference-Format}

\begin{document}
\title{How Slow do my Transient Typechecks Go?}

\author{Isaac Oscar Gariano}
\affiliation{
  \department{Engineering and Computer Science} % \department is recommended
  \institution{Victoria University of Wellington}
  \country{New Zealand}
}
\email{Isaac@ecs.vuw.ac.nz} % \email is recommended

\author{Richard Roberts}
\affiliation{
  \department{Computational Media Innovation Centre} % \department is recommended
  \institution{Victoria University of Wellington}
  \country{New Zealand}
}
\email{rykardo.r@gmail.com} % \email is recommended


\author{Stefan Marr}
\orcid{0000-0001-9059-5180}
\affiliation{
  \department{School of Computing} % \department is recommended
  \institution{University of Kent}
  \country{United Kingdom}
}
\email{s.marr@kent.ac.uk} % \email is recommended


\author{Michael Homer}
\affiliation{
  \department{Engineering and Computer Science} % \department is recommended
  \institution{Victoria University of Wellington}
  \country{New Zealand}
}
\email{mwh@ecs.vuw.ac.nz} % \email is recommended



\author{James Noble}
\orcid{0000-0001-9036-5692}             %% \orcid is optional
\affiliation{
  \department{Engineering and Computer Science} % \department is recommended
  \institution{Victoria University of Wellington}
  \country{New Zealand}
}
\email{kjx@ecs.vuw.ac.nz} % \email is recommended




\begin{abstract}
One form of type checking used in gradually typed language is
\emph{transient type checking}: whenever an object `flows' through
code with a type annotation, the object is dynamically checked to
ensure it has the methods required by the annotation.

Although na\"ive implementations of transient type checks have a high runtime overhead, just-in-time compilation and optimisation in virtual machines can eliminate much of this overhead.  Unfortunately
the improvement is not uniform: while most type checks can be optimised
away, some will significantly decrease a program's
performance, and some may even increase it.  In this paper we investigate techniques that interpret benchmark results to identify which type checks have the greatest effects on performance.
Using these techniques,
programmers can optimise programs by removing expensive type checks,
and VM engineers can identify new opportunities for compiler optimisation.   

\sm{this is much to brief for me.
Transient typechecks is a very specific thing.
I believe, VM people need more context here.
It's a form of gradual typing, which requires comparably simple checks
at run time to check type violations.
Then, how do we investigate the issues? we should give a hint what the solution is. otherwise, the sentence in using these techniques doesn't make sense to me, because we didn't say anything about them.}
\kjx{still like brief abstracts. at least I defined ``techniques''}
\end{abstract}


%% Generate at 'http://dl.acm.org/ccs/ccs.cfm'.
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10011007.10011006.10011041.10011044</concept_id>
<concept_desc>Software and its engineering~Just-in-time compilers</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10011007.10011006.10011008.10011009.10011011</concept_id>
<concept_desc>Software and its engineering~Object oriented languages</concept_desc>
<concept_significance>300</concept_significance>
</concept>
<concept>
<concept_id>10011007.10011006.10011041.10010943</concept_id>
<concept_desc>Software and its engineering~Interpreters</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~Just-in-time compilers}
\ccsdesc[300]{Software and its engineering~Object oriented languages}
\ccsdesc[300]{Software and its engineering~Interpreters}

\keywords{dynamic type checking, gradual types, optional types, Grace,
Moth, object-oriented programming}
\maketitle


\section{Introduction}


Gradual Typing aims to add static type annotations to dynamic languages, increasing
their safety while maintaining flexibility
\citep{GiladPluggable2004,Siek2006,XXXSiek2015}
--- or, complementarily,
to permit dynamic type annotations within static languages, increasing
flexibility whilst maintaining safety
\citep{AbadiTOPLAS1991}.

There is a spectrum of different approaches to gradual typing
\cite{kafka18,bensurvey18icfp}.
At one end is ``pluggable types'' (as in Strongtalk \cite{strongtalk}) or ``erasure
semantics'' (as in 
%% Two important approaches are optional\citep{}
%% and gradual typing\citep{GiladPluggable2004,Siek2006,XXXSiek2015}.
%% These are applied to dynamic languages to reap the benefits of typing, but
%% unfortunately also have limitations.
%With optional or pluggable approaches such as 
%\citeurl{TypeScript,}{TypeScript}{Microsoft}{27 June
%2018}{https://www.typescriptlang.org/}
TypeScript
\citep{typeScriptECOOP}), where
%
all types are erased before execution, limiting the benefit of
types to statically typed portions of programs, while not providing 
any for of run time checking.  In the middle there are
``transient'' or ``type-tag'' checks (as in Reticulated Python),
% and ``concrete'' checks as in Thorn
which offer first-order semantics, by checking
that an object's type constructor or supported methods match
% any explicit type annotations
any static types
that the object flows through.
%%%WRONG kjx
%, and checking only when data flows through those declarations
\cite{Siek2007,Bloom2009,concrete15,reticPython2014,Greenman2018}.
Reticulated Python also supports alternative ``monotonic'' semantics
which mutate objects by narrowing their concrete types when passed
into contexts with more specific types.
On the other end of the spectrum are behavioural
typechecks (as in Typed Racket \cite{typedScheme08,takikawa2012},
Gradualtalk \cite{gradualtalk14}),
and ``proxies'' (provided by Reticulated Python), which
support higher-order semantics, retaining
types until run 
time, and performing checks eagerly, thus detailed information
about type violations can be given as soon as possible via ``blame''
tracking \cite{blame2009,blameForAll2011}.
Finally, there is also ductile typing,
which dynamically interprets a static type system at run time
\cite{Ductile2011}.
%
%\mwh{Does a monotonic semantics fit on this spectrum somewhere?}
%\kjx{Not in the greenman/fellenstein spectrum, but I guess I'd put it
%  after concrete and before behavioural. Does it matter?}
%
% This said "any", but the ecoop paper specifically showed that 
Unfortunately, most gradual systems with run-time semantics
(as opposed to type erasure as in TypeScript) typically
imposes significant run-time performance overheads to provide these
semantics.  This has lead to a significant body of research,
on one hand developing techniques to optimise gradual typing 
\citep{Vitousek2017,Muehlboeck2017,Bauman2017,Richards2017,Greenman2018}
and on the other, developing techniques to evaluate the
resulting performance  \cite{Takikawa2016,Greenman2019jfp}.


This paper builds upon our recent work on optimising transient
type checks \cite{Roberts2017,roberts-and-co-ecoop-2019} by
investigating how benchmark results can be repurposed to identify which
particular transient type checks are resistant to optimisation.
The next section discusses dynamic type checks and gradual typing in
Moth (an implementation of the Grace language), then section~\ref{s-eval} describes our benchmarking protocol. Section~\ref{s-overall} then presents the overall results of
our benchmarks, while section~\ref{s-individual}
looks at the results of benchmarking individual type checks.
Section~\ref{s-rel} presents some additional related work, and finally \ref{s-concl} summarises our results, and briefly considers threats to validity.

\section{Background}
\label{s-bg}

Our work is based on the Moth virtual machine 
\cite{Roberts2017,roberts-and-co-ecoop-2019},
an implementation
of the Grace programming language 
\citep{graceOnward12,graceSigcse13}.
Moth is based on the Graal and Truffle toolchain
\cite{Wurthinger:2017:PPE,Wurthinger2013},
and developed from a Newspeak implementation based on the  Simple
Object Machine \cite{Daloze2016,SOMns}.

\subsection{Grace and Transient Type Checking}

Grace is an object-oriented, imperative, educational programming
language, with a focus on introductory programming
courses, but also intended for more advanced study and research \citep{graceOnward12,graceSigcse13}.
%
While Grace's syntax draws
from the so-called ``curly bracket'' traditions of C, Java, and
JavaScript, the structure of the language
is in many ways closer to Smalltalk:
all computation is done via dynamically dispatched  ``method requests''
where the object receiving the request decides what code to run,
control structures are built out of lambda expressions support ``non-local'' returns, i.e. they can return to the point where execution first encountered the lambda \citep{bluebook}.  In
other ways, Grace is closer to JavaScript than Smalltalk: Grace
objects are created from object literals, rather than by
instantiating classes \citep{Black2007-emeraldHOPL,JonesECOOP2016} and
objects and classes can be deeply nested within each 
other \citep{betabook}.

\paragraph{Grace's Typing}
In Grace, all declarations can be annotated with types.
As Grace is designed to support a variety of teaching methods, implementation of Grace are free to check such type annotations  statically, dynamically, or not at all.
The type system of Grace is intrinsically gradual:%
%
~type annotations should not affect the semantics of a correct
program \citep{XXXSiek2015}. The type system
includes a distinguished ``{Unknown}'' type which matches any other type; this unknown type is the default when type annotations are omitted.

Static typing for the core of Grace's type system has been described
elsewhere \citep{TimJonesThesis};
here we explain how
these types can be understood 
dynamically, from the Grace programmer's point of view.
Grace's types are structural \citep{graceOnward12},
that is, an object conforms to a type whenever it conforms to the ''structural`` requirements of a type,
rather than requiring classes or objects to explicitly declare their intended type.

In Grace, types specify a set of method signatures that an object must provide. A type expresses the requests an object can respond to, for example whether a particular accessor is available, rather than a location in a class hierarchy.

\paragraph{Moth's Transient Type Checking}
Moth's implementation of transient type checks are only only first-order.
Moth only checks dynamically that an object has methods of the same name and arity as are required by a type:  any argument and return types of such methods are not checked.

In particular, Moth performs the following type checks at run time:
\begin{itemize}
\item when a method is requested, arguments that are passed are checked against the corresponding parameter type annotations of the called method, this is done before the body of the method is executed;
\item when the body of a method has finished executing, but before it returns to its caller, the method's return value is checked against the return type annotation of the called method;
\item whenever a variable is read or written to, its value is checked against the type specified by the variables declaration.
\end{itemize}

To see how this works in practice, consider this piece of Grace code:

\begin{minipage}{\linewidth}
\begin{lstlisting}
def o = object {
   method three -> Number {3}
}
type ThreeString = interface {
   three -> String
}
def t : ThreeString = o
printNumber (t.three)
\end{lstlisting}
\end{minipage}

Moth will perform dynamic type checks:

\begin{itemize}

\item on line 7,
when the \code{o} object initialises the variable \code{t},
Moth checks that \code{o} has a 0-argument method called \code{three};

\item on line 8,
when the value of \code{t} is read,
Moth checks that its value (\code{o}) still has a \code{three} method;

\item on line 2,
when the method requested by ``\code{t.three}'' returns,
Moth checks that returned value conforms to the \code{Number} type;
and (presumably) within the definition of
%
\code{printNumber(n :   Number)}
%
(not shown), Moth will again check that the value is a \code{Number}.
\end{itemize}

Note that we never check
whether the result of requesting ``\code{t.three}'' is actually
a \code{String} (as one may expect from line 5) because Moth only performs first-order type checks
(it checks whether objects have conforming methods) not higher-order
checks (whether the argument and result types of methods' conform). In addition, Moth
only checks when values flow through explicit type annotations.
This is why the type declared in lines 4-6 is checked only on line 7
(where it is mentioned explicitly); and the check only requires the
presence of a method called \code{three}, regardless of the method's
declared return type.

\paragraph{Moth's Optimisation}
We are developing Moth as a
research platform \cite{roberts-and-co-ecoop-2019}. Like other VMs
based on the Truffle and Graal toolchain, Moth is a self-optimising
AST interpreter \cite{Wurthinger:2012:SelfOptAST}. 
The key idea is that an AST rewrites itself based on a program's run time values
to reflect the minimal set of operations needed to execute the program
correctly. The rewritten AST is then compiled into efficient machine
code. This rewriting often depends on the dynamic types of the
objects involved. In the simplest case, a ``self'' call (when one method
on an object requests a second method on the exact same object) will
always result in executing the exact same method. Thus the called method can be inlined into
the callee, avoiding overhead of a machine-level subroutine
invocation and an object-oriented dynamic dispatch.


Moth relies on a number of standard techniques for optimising
object-oriented programs.
``Shapes'' \citep{woss2014object} capture information about objects'
structures and (run time) 
field types, allowing a just-in-time compiler to
represent objects in memory similarly to C structs and, consequently,
can generate highly efficient code.
``Polymorphic inline caches''
\citep{Hoelzle:91:PIC} use object shapes to cache the results of
method lookups, avoiding expensive class hierarchy searches or
indirect jumps through virtual method tables. 
Since Moth is built on the Truffle framework,
Graal comes with  additional support for partial evaluation,
which enables efficient native code generation for
Truffle interpreters \citep{Wurthinger:2017:PPE}.
\RR{This last sentence seems out of place, does it belong here?}
\kjx{yes I think it's fine}

\section{Evaluating Transient Typechecks}
\label{s-eval}

In our previous work \cite{roberts-and-co-ecoop-2019} we evaluated the
performance of transient type checks using 21 benchmarks.
We started with the 14 benchmarks of the \AWFY
benchmark suite \cite{Marr2016}: Bounce, CD, DeltaBlue, GraphSearch, Havlak, Json, List, Mandelbrot, NBody, Permute, Richards, Sieve, Storage and Towers benchmarks. We added another nine benchmarks taken from the gradual-typing literature.
Our goal was to complement the \AWFY with additional ones that are used for similar experiments and can be ported to Grace.
We surveyed a number of papers
\citep{Takikawa2016,Vitousek2017,Muehlboeck2017,Bauman2017,Richards2017,Stulova2016,Greenman2018}
and selected additional benchmarks that have been used by multiple papers.
Two of these benchmarks (NBody and Sieve) overlapped with the \AWFY suite,
or were available in different versions.
While not always behaviourally equivalent,
we chose the \AWFY versions since we already used them to
establish a performance baseline.
The selected benchmarks as well as the papers in which they were used are shown in
Table~\ref{tab:gradual-benchmarks}.

\begin{table}[htb]
  \caption{Benchmarks selected from literature.}
  \label{tab:gradual-benchmarks}
  \begin{center}
    \begin{tabular}{l l r}
      Fannkuch & \cite{Vitousek2017,Greenman2018} \\
      Float & \cite{Vitousek2017,Muehlboeck2017,Greenman2018} \\
      Go & \cite{Vitousek2017,Muehlboeck2017,Greenman2018} \\
      NBody & \cite{Kuhlenschmidt:2018:preprint,Vitousek2017,Greenman2018} & used \cite{Marr2016} \\
      Queens & \cite{Vitousek2017,Muehlboeck2017,Greenman2018} & used \cite{Marr2016} \\
      PyStone & \cite{Vitousek2017,Muehlboeck2017,Greenman2018} \\
      Sieve & \cite{Takikawa2016,Muehlboeck2017,Bauman2017,Richards2017,Greenman2019jfp} & used \cite{Marr2016} \\
      Snake & \cite{Takikawa2016,Muehlboeck2017,Bauman2017,Richards2017,Greenman2019jfp} \\
      SpectralNorm & \cite{Vitousek2017,Muehlboeck2017,Greenman2018} \\
    \end{tabular}
  \end{center}
\end{table}

The benchmarks were modified to have complete type information.  To
ensure correctness and completeness of these experiments, we ensured that each benchmark is completely typed.  To assess the performance
overhead of type checking, we compared the execution of Moth with all
checks disabled against an execution that has all checks enabled.  The
results of this evaluation demonstrated that Moth's transient type checking has an overhead of 5\% (min.\ -13\%,
max. 79\%) compared to Moth's peak performance with type checking disabled.

We did not measure programs that mixed typed and untyped code because
with our implementation technique we expected fully typed programs to
have the largest overhead. This is not the case for other kinds of
gradual type checking, which have given rise to more complex
evaluation techniques.  In particular, the so-called ``Takikawa'' or
``Takikawa-Greenman'' evaluation protocol \cite{Takikawa2016,Greenman2019jfp}
benchmarks a \emph{lattice} of $2^N$ \emph{configurations} of each
benchmark, where a configuration is a particular mix of static and
dynamically typed code.

The Takikawa evaluation protocol was originally proposed for Typed
Racket, where static vs dynamic typing is set per-module, so $N$ is
the number of modules. Grace, following other languages such as
Reticulated Python \cite{reticPython2014,monotonic2015,Vitousek2017},
allows programmers to choose whether each individual declaration should be type-checked. This means $N$ in Grace is the number of type annotations in the program, and so
checking an entire lattice for even a moderately sized benchmark would be infeasible.  Vitousek et al.\ modified the Takikawa protocol for
these kinds of languages by taking an approach based on sampling
\cite{vitousek-transient-arXive-2019}.  The Takikawa-Vitousek protocol
divides the number of type annotations in a fully-typed program into
a maximum of 100 intervals, and then randomly generates ten programs within
each interval by erasing type annotations.

To investigate Moth's support for transient type checks further, we carried out a similar approach to the Takikawa-Vitousek protocol. For each benchmark we generated 100 partially typed versions (fewer if the benchmark was too small). We did an even split so that for each interval $i \ge 1$ and $i$ < $N$, we generated roughly the same number of configurations with $i$ type annotations. We used Robert Floyd's sampling algorithm \cite{Bentley:1987:PPS:30401.315746} to choose the type annotations each configuration contained randomly, and we ensured that no duplicate configurations were generated. In addition to these, we tested fully untyped and typed versions, for a total of 102 configurations per benchmark, (or 97 in the case of Storage, since it only has 10 type annotations).

As in our previous work \cite{roberts-and-co-ecoop-2019} which used the same set of benchmarks, to account for the complex warmup behaviour of modern systems \citep{Barrett:2017:VMW} as well as
the non-determinism caused by \eg garbage collection and cache effects, we ran each benchmark for 1000 iterations in the same invocation of Moth, and discard the first 350 iterations to ignore warmup JIT compilation.

Though outliers remain visible in the plots for each individual benchmark, the largest 95\% confidence interval we obtained (over the mean time after warmup) for any of experiments was $\pm3.53\%$ (for the PyStone benchmark).

All experiments were executed on a Windows 10 machine
running Fedora 30 within Windows Subsystems for Linux (version 1). The machine has 
an Intel Core i7-6800K @3.40GHz, with 6 cores, for a total of 12 hyperthreads. We used ReBench 1.0 \citep{ReBench:2018}, Java 1.8.0\_191 Graal 0.43.
Benchmarks were executed one by one to avoid interference between them.
The analysis of the results and plots where generated using Python 3.7.3 and PGFPLOTS 1.16.
To enable reproductions, the scripts we used to generate and run our experiments, including the source code for all the configurations tested, are available online.\footnote{
\url{https://gitlab.ecs.vuw.ac.nz/isaac/Moth-Takikawa}}


\section{Results}
\label{s-overall}

\begin{figure*}
	\input{graphs/full-scatter}
	\caption{Graphs of (at most) 102 configurations in the typing lattices for each benchmark. Time is measured as the mean of the 351\textsuperscript{st} to the 1,000\textsuperscript{th} benchmark iteration under a single invocation of Moth (lower is better).}
	\label{f:full}
\end{figure*}
The results for the performance of our sample of typing lattice configurations are presented in figure~\ref{f:full}. Following \cite{vitousek-transient-arXive-2019}, the points on each graph show the average execution of each individual configuration. The x-axis represents the proportion of type annotations for each configuration, with the left- and right-most points showing the times for the fully untyped and typed configurations respectively. The execution time in milliseconds is shown on the left y-axes, and time relative to the fully untyped configuration is shown on the right y-axes.

Most of the graphs are essentially horizontal lines, indicating that the overhead of including type annotations is negligible. The plots for CD and Richards show a roughly linear increase, however, i.e.\ for these two benchmarks, adding type annotations reduces performance linearly. On the other hand, the plots for Go, Permute, DeltaBlue, and Storage show \emph{decreases}: i.e. adding more type annotations \emph{improves} performance of these benchmarks.

Of particular note is that some of the graphs (Permute, Storage, Snake, Towers, and Richards) appear to show bimodal performance profiles, that is, two separate roughly horizontal lines. Presumably Moth can remove all the overhead from some configurations, but in others there must be one or more type checks that cannot be optimised away. List appears to have three performance modes: most of the configurations up to about 60\% annotations perform at roughly the same speed as untyped code; most configurations above 70\% perform 1.8 times more slowly than untyped code; and then across all annotation percentages there are configurations that perform roughly 1.5 times slower than the untyped baseline.


\section{Individual Annotations}
\begin{figure*}
	\input{graphs/pattern}
	\caption{Graphs of typing latices and performance with only an individual type annotation.}
	\label{f:pattern}		
\end{figure*}

We hypothesise that the tri/bi-modal graphs shown in figure~\ref{f:full} are caused by individual type annotations: that is, particular annotations have a significant effect on performance. In order to investigate this, we ran further experiments by testing configurations with each \emph{individual} type annotation enabled, and all others removed, and we then used this information to approximate and see whether this appears to show a relation with the results across the typing lattice.

Firgure X shows the patterns we were able to identify, in particular...



\label{s-individual}

We hypothesised that the overhead of an individual type annotation may
be able to predict the performance of program configurations with that
annotation. To test this hypothesis, we constructed a series of
configurations, one for each type annotation, with only that
one annotation included.



   trying-to-be-cleverer results

   - single annotation overheads
  - using them go back to scatterplots (i.e. what you do in the latest graphs) 

  - show that single annotation prediction doesn't really work\ldots

\section{Related Work}
\label{s-rel}

Optimization Coaching
\citep{St-Amour:2012:OCO} is a related approach more
directly integrated into the language runtime and development environment.
Optimizer Coaching uses feedback from the runtime to guide developers
to insert or change type declarations 
to enable a compiler to generate a more optimal program.
In this spirit, we would eventually want to achieve the same, although
in our case,
we need to run full experiments to get the necessary information.
Furthermore, we still lack the seamless integration in a development environment.

The high-performance computing community also has been investigating how
tools and visualization can help developers to utilize their systems
more efficiently \citep{Papenhausen:2016:IVT,daSilva:2019:PSV}.
Their focus is typically on parallelisation opportunities,
guided by runtime feedback, cost models, or heuristics. 
The large body of work \citep{Isaacs:2014:PerfViz} uses various approaches,
though, we are not aware of work that has used an approach similar to ours.
One may argue, this is largely a question of practicality.\sm{
i put this in to hint at issues with usability.}
\kjx{Hmm not sure what the last sentence here means!}



\section{Discussion and Conclusion}
\label{s-concl}

In this paper we have investigated how benchmark results can be
repurposed to determine precisely which transient typechecks are
resistant to the optimisations provided by a just-in-time compiling
virtual machine.  We observed that many of our benchmark results
conducted under the Takikawa protocol 
seemed to have a characteristic bimodal performance profile: some
of the benchmark configurations ran significantly slower than
the remaining configurations. By inspecting the benchmark graphs coded
for each typecheck in a program's source code, we were often able to
identify that just one or two of the typechecks were responsible for
the bimodal performance.  Based on that result, we also investigated
individual benchmark configurations for each individual typecheck:
these results can identify problematic typechecks, in some but not all cases. 

This is preliminary work: there are a number of threats to validity. 
Regarding construct validity, 
our underlying
implementation may contain undetected bugs that affect the semantics
or performance of the gradual typing checks. Regarding internal
validity,
our benchmarking harness runs on the same implementation
and therefore is subject to the same issues.
Regarding external validity, 
\RR{The next two sentences seem to be in the wrong place (they state an advantage rather than a threat); maybe move them into the previous paragraph?}
\kjx{I'm happy with them here about external validity}
Moth is built
on the Truffle and Graal toolchain, so we expect
to resemble other Graal
VMs doing similar AST-based optimizations of transient typechecks.
Because we rely on common techniques, 
we expect our results should be transferable to JIT implementations
more widely.


Finally, it is not clear how our results would transfer
to other gradually typed-languages or other semantics for gradual typing.
Our benchmarks do not depend on any features of Grace
that are not common in other gradually-typed object-oriented
languages, but as Grace lacks a large corpus of programs the
benchmarks are necessarily small and artificial.
The advantage of Grace and Moth for this research is
that their relative simplicity means we have been able to build an
implementation that features competitive performance with significantly less
effort than would be required for larger and more complex languages.

In the future, we hope to investigate statistical techniques to
determine the significance of each type annotation's contribution to a
programs overall performance. We would also like to investigate 
whether this approach can assist with optimsations for programmers' day-to-day  development, or help VM engineers identifying performance bugs in the underlying virtual machines.

\sm{is our approach practical. Is it useful for day-to-day development?
What's its context, use, general application?}
\kjx{filled out the last sentence}

\bibliography{references}


\end{document}
