\documentclass[sigplan,10pt,review,screen]{acmart}\settopmatter{printfolios=true}

\acmConference[VMIL'19]{ACM SIGPLAN International Workshop on Virtual Machines and Intermediate Languages}{October 22, 2019}{Athens, Greece}
\acmYear{2019}
\acmISBN{978-x-xxxx-xxxx-x/YY/MM}
\acmDOI{10.1145/nnnnnnn.nnnnnnn}
\startPage{1}


\def\AWFY{Are\,We\,Fast\,Yet\xspace}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\ie}{i.e.\xspace}
\newcommand{\eg}{e.g.\xspace}

\usepackage{listings}
\usepackage{xspace}


\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{codered}{rgb}{0.82,0.15,0.23}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

 
\lstdefinestyle{mystyle}{
    language=SAS,
    breakatwhitespace=true,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2,
    aboveskip=20pt,
    belowskip=10pt,
    xleftmargin=0.5cm,
    basicstyle=\footnotesize\ttfamily,
    commentstyle=\itshape\color{codegray},
    numberstyle=\footnotesize\color{codegray},
    stringstyle=\color{codegreen},
    keywordstyle = {\color{codepurple}},
    keywordstyle = [2]{\color{codered}},
    keywordstyle = [3]{\color{codered}},
    keywordstyle = [4]{\color{codegreen}},
    keywords={method,object,interface,type,var,def,return,class,for,in,if},
    otherkeywords = {:,->},
    morekeywords = [2]{:},
    morekeywords = [3]{->},
    morekeywords = [4]{true,false},
    morestring=*[d]{"},
    backgroundcolor={}
}

\lstset{style=mystyle}

\setcopyright{none}
\bibliographystyle{ACM-Reference-Format}

\begin{document}
\title{How Slow do my Transient Typechecks Go?}

\author{Isaac Oscar Gariano}
\affiliation{
  \department{Engineering and Computer Science} % \department is recommended
  \institution{Victoria University of Wellington}
  \country{New Zealand}
}
\email{Isaac@ecs.vuw.ac.nz} % \email is recommended

\author{Richard Roberts}
\affiliation{
  \department{Computational Media Innovation Centre} % \department is recommended
  \institution{Victoria University of Wellington}
  \country{New Zealand}
}
\email{rykardo.r@gmail.com} % \email is recommended


\author{Stefan Marr}
\affiliation{
  \department{School of Computing} % \department is recommended
  \institution{University of Kent}
  \country{s.marr@kent.ac.uk}
}
\email{mwh@ecs.vuw.ac.nz} % \email is recommended


\author{Michael Homer}
\affiliation{
  \department{Engineering and Computer Science} % \department is recommended
  \institution{Victoria University of Wellington}
  \country{New Zealand}
}
\email{mwh@ecs.vuw.ac.nz} % \email is recommended



\author{James Noble}
\orcid{0000-0001-9036-5692}             %% \orcid is optional
\affiliation{
  \department{Engineering and Computer Science} % \department is recommended
  \institution{Victoria University of Wellington}
  \country{New Zealand}
}
\email{kjx@ecs.vuw.ac.nz} % \email is recommended




\begin{abstract}
Just-in-time compilation and optimisation in virtual machines can
eliminate much of the overhead of transient typechecks.  Unfortunately
the improvement is not uniform: while most typechecks can be optimised
away, some typechecks will will significantly decrease a program's
performance.  In this paper we investigate how benchmark results can be
indicate which typechecks cause the most problems. Programmers could
use these techniques to optimise their programs by removing
typechecks, and VM engineers to identify opportunities for further
optimisations.   
\end{abstract}


%% Generate at 'http://dl.acm.org/ccs/ccs.cfm'.
%%KJX XML IS BROKEN!!!
\begin{CCSXML}
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~Just-in-time compilers}
\ccsdesc[300]{Software and its engineering~Object oriented languages}
\ccsdesc[300]{Software and its engineering~Interpreters}

\keywords{dynamic type checking, gradual types, optional types, Grace,
Moth, object-oriented programming}
\maketitle


\section{Introduction}


Gradual Typing aims to add static type annotations to dynamic languages, increasing
their safety while maintaining flexibility
\citep{GiladPluggable2004,Siek2006,XXXSiek2015}. 
--- or, complementarily,
to permit dynamic type annotations within static languages, increasing
flexibility while maintaining safety
\citep{AbadiTOPLAS1991}.

There is a spectrum of different approaches to gradual typing
\cite{kafka18,bensurvey18icfp}.
At one end\,---\,``pluggable types'' as in Strongtalk \cite{strongtalk} or ``erasure
semantics'' as in 
%% Two important approaches are optional\citep{}
%% and gradual typing\citep{GiladPluggable2004,Siek2006,XXXSiek2015}.
%% These are applied to dynamic languages to reap the benefits of typing, but
%% unfortunately also have limitations.
%With optional or pluggable approaches such as 
%\citeurl{TypeScript,}{TypeScript}{Microsoft}{27 June
%2018}{https://www.typescriptlang.org/}
TypeScript\citep{typeScriptECOOP}\,---\,
%
all types are erased before the execution, limiting the benefit of
types to the statically typed parts of programs, and preventing
programs from depending on type checks at run time.  In the middle,
``transient'' or ``type-tag'' checks as in Reticulated Python 
% and ``concrete'' checks as in Thorn
offer first-order semantics, checking
whether an object's type constructor or supported methods match
explicit type declarations
%%%WRONG kjx
%, and checking only when data flows through those declarations
\cite{Siek2007,Bloom2009,concrete15,reticPython2014,Greenman2018}.
Reticulated Python also supports an alternative ``monotonic'' semantics
which mutates an object to narrow its concrete type when it is passed
into a more specific type context.
At the other end of the spectrum, behavioral
typechecks as in Typed Racket \cite{typedScheme08,takikawa2012},
Gradualtalk \cite{gradualtalk14},
and Reticulated Python's proxies,
support higher-order semantics, retaining
types until run 
time, performing the checks eagerly, and giving detailed information
about type violations as soon as possible via blame
tracking \cite{blame2009,blameForAll2011}.
Finally, Ductile typing
dynamically interprets a static type system at runtime
\cite{Ductile2011}.
%
%\mwh{Does a monotonic semantics fit on this spectrum somewhere?}
%\kjx{Not in the greenman/fellenstein spectrum, but I guess I'd put it
%  after concrete and before behavioural. Does it matter?}
%
Unfortunately, any gradual system with run-time semantics
(i.e.\ everything more complex than erasure) currently
imposes a significant run-time performance overhead to provide those
semantics.  This has lead to a significant body of research,
on one hand developing techniques to optimise gradual typing 
\citep{Vitousek2017,Muehlboeck2017,Bauman2017,Richards2017,Greenman2018}
and on the other hand developing techniques to evaluate the
resulting performance  \cite{Takikawa2016,Greenman2019jfp}.


This paper builds upon our recent work on optimising transient
typechecks \cite{Roberts2017,roberts-and-co-ecoop-2019} to
investigate how benchmark results can be repurposed to identify which
particular transient typechecks are resistant to optimisation.
The next section discusses dynamic typechecks and gradual typing in
Grace, then section~\ref{s-eval} describes the benchmarking
protocols. Section~\ref{s-overall} then presents the results of
analysing overall benchmark results, while section~\ref{s-individual}
looks at the results of benchmarking individual type checks.
Finally~\ref{s-concl} discusses our results, briefly considers threats
to validity, and concludes.

\section{Background}
\label{s-bg}

Our work is based on Moth virtual machine 
\cite{Roberts2017,roberts-and-co-ecoop-2019}
an implementation
of the Grace programming language 
\citep{graceOnward12,graceSigcse13},
based on the Graal and Truffle toolchain
\cite{Wurthinger:2017:PPE,Wurthinger2013}
and developed from a Newspeak implementation based on the  Simple
Object Machine \cite{Daloze2016,SOMns}.

Grace is an object-oriented, imperative, educational programming
language, with a focus on introductory programming
courses, but also intended for more advanced study and research \citep{graceOnward12,graceSigcse13}.
%
While Grace's syntax draws
from the so-called ``curly bracket'' tradition of C, Java, and
JavaScript, the structure of the language
is in many ways closer to Smalltalk:
all computation is via dynamically dispatched  ``method requests''
where the object receiving the request decides which code to run,
and
%
returns within lambdas that are ``non-local'', returning to the method
activation in which the block is instantiated \citep{bluebook}.  In
other ways, Grace is closer to JavaScript than Smalltalk: Grace
objects can be created from object literals, rather than by
instantiating classes \citep{Black2007-emeraldHOPL,JonesECOOP2016} and
objects and classes can be deeply nested within each 
other \citep{betabook}.

Critically, Grace's declarations and methods' arguments
and results can be annotated with types, and those types can be  checked
either statically or dynamically. This means the type system is
intrinsically gradual:%
%
~type annotations should not affect the semantics of a correct
program\citep{XXXSiek2015}, and the type system
includes a distinguished ``{Unknown}'' type which matches any other type
and is the implicit type for untyped program parts.

The static core of Grace's type system is well described
elsewhere\citep{TimJonesThesis};
here we explain how
these types can be understood 
dynamically, from the Grace programmer's point of view.
Grace's types are structural \citep{graceOnward12},
that is, an object implements a type whenever it
implements all the methods required by that type,
rather than requiring classes or objects to declare types explicitly.
Grace's transient dynamic typechecks are first order, so methods match when they have
the same name and arity: 
argument and return types are ignored.
A type thus expresses the requests an object can respond to,
for example whether a particular accessor is available,
rather than a nominal location in an explicit inheritance hierarchy.

Grace then checks the types of values at run time:
%
\begin{itemize}
\item the values of arguments are checked after a method is requested, 
      but before the body of the method is executed;
\item the value returned by a method is checked after its body is executed; and
\item the values of variables are checked
      whenever written or read by user code.
\end{itemize}

To see how this works in practice, consider this code snippet, which
will run in Grace.

\begin{lstlisting}
def o = object {
   method three -> Number {3} }
type ThreeString = interface {
   three -> String
}
def t : ThreeString = o
printNumber (t.three)
\end{lstlisting}

Dynamic typechecks will take place: on line 6, when object \code{o}
initialises the variable \code{t}, we check it has the method
\code{three}; on line 7, when the value of \code{t} is read, it must
again have the method \code{three}; on line 2, when the \code{three}
method requested by ``\code{t.three}'' returns, the return value must
be a \code{Number}; and (presumably) within the definition of 
%
\code{printNumber(n :   Number)} 
%
(not shown) to check that its argument is a number. Note that we never check
whether e.g.\ the result of requesting ``\code{t.three}'' is actually
a \code{String} (as one may expect from line 4) because we check
only first-order types (whether objects have conforming methods) not higher-order
types (whether those methods' argument and result types conform), and
we check only when values flow through explicit type annotations.
This is why the type declared in lines 3-5 is checked only on line 6
(where it is mentioned explicitly); and the check only requires the
presence of a method called \code{three}, regardless of the method's
declared return type.

Moth is a virtual machine for Grace that we are developing as a
research platform \cite{roberts-and-co-ecoop-2019}. Like other VMs
based on the Truffle and Graal toolchain, Moth is a self-optimising
AST interpreter \cite{Wurthinger:2012:SelfOptAST}. 
The key idea is that an AST rewrites itself based on a program's run-time values
to reflect the minimal set of operations needed to execute the program
correctly. The rewritten AST is then compiled into efficient machine
code. This rewriting often depends on the dynamic types of the
objects involved. In the simplest case, a ``self'' call (when one method
on an object invokes a second method on the exact same object) will
always result in executing the second method defined in same concrete
class as the first method. The second method can then be inlined into
the first method, avoiding the overheads of a machine-level subroutine
invocation and of an object-oriented dynamic dispatch.


Moth relies on a number of standard techniques for optimising
object-oriented programs.
''Shapes'' \citep{woss2014object} capture information about objects'
structure and (runtime) 
field types, allowing a just-in-time compiler to
represent objects in memory similarly to C structs, and so to 
generate highly efficient code.
``Polymorphic inline caches''
\citep{Hoelzle:91:PIC} uses object shapes to cache the results of
method lookups, avoiding expensive class hierarchy searches or
indirect jumps through virtual method table. 
For languages built on the Truffle framework,
Graal comes with  additional support for partial evaluation,
which enables efficient native code generation for
Truffle interpreters\citep{Wurthinger:2017:PPE}.



\section{Evaluating Transient Typechecks}
\label{s-eval}

In our previous work \cite{roberts-and-co-ecoop-2019} we evaluated the
performance of transient typechecks using the \AWFY
benchmark suite, plus other
benchmarks from the gradual-typing literature.
The goal was to complement our benchmarks with additional ones that are
used for similar experiments and can be ported to Grace.
To this end, we surveyed a number of papers\citep{Takikawa2016,Vitousek2017,Muehlboeck2017,Bauman2017,Richards2017,Stulova2016,Greenman2018}
and selected benchmarks that have been used by multiple papers.
Some of these benchmarks overlapped with the \AWFY suite,
or were available in different versions.
While not always behaviorally equivalent,
we chose the \AWFY versions since we already used them to
establish the performance baseline.
The selected benchmarks as well as the papers in which they were used are shown in
Table~\ref{tab:gradual-benchmarks}.

\begin{table}[htb]
  \caption{Benchmarks selected from literature.}
  \label{tab:gradual-benchmarks}
  \begin{center}
    \begin{tabular}{l l r}
      Fannkuch & \cite{Vitousek2017,Greenman2018} \\
      Float & \cite{Vitousek2017,Muehlboeck2017,Greenman2018} \\
      Go & \cite{Vitousek2017,Muehlboeck2017,Greenman2018} \\
      NBody & \cite{Kuhlenschmidt:2018:preprint,Vitousek2017,Greenman2018} & used \cite{Marr2016} \\
      Queens & \cite{Vitousek2017,Muehlboeck2017,Greenman2018} & used \cite{Marr2016} \\
      PyStone & \cite{Vitousek2017,Muehlboeck2017,Greenman2018} \\
      Sieve & \cite{Takikawa2016,Muehlboeck2017,Bauman2017,Richards2017,Greenman2019jfp} & used \cite{Marr2016} \\
      Snake & \cite{Takikawa2016,Muehlboeck2017,Bauman2017,Richards2017,Greenman2019jfp} \\
      SpectralNorm & \cite{Vitousek2017,Muehlboeck2017,Greenman2018} \\
    \end{tabular}
  \end{center}
\end{table}

The benchmarks were modified to have complete type information.  To
ensure correctness and completeness of these experiments, we added an
additional check to Moth that reports absent type information to
ensure each benchmark is completely typed.  To assess the performance
overhead of type checking, we compared the execution of Moth with all
checks disabled against an execution that has all checks enabled.  The
results of this evaluation demonstrated that Moth run programs with
transient typechecks enabled at an overhead of of 5\% (min.\ -13\%,
max. 79\%) over peak performance without typechecks.

We did not measure programs that mix typed and untyped code because
with our implementation technique a fully typed program is expected to
have the largest overhead. This is not the case for other kinds of
gradual type checking, which have given rise to more complex
evaluation techniques.  In particular, so-called ``Takikawa'' or
``Takikawa-Greenman'' evaluation \cite{Takikawa2016,Greenman2019jfp}
benchmarks a \emph{lattice} of $23^N$ \emph{configurations} of each
benchmark. where a configuration is a particular mix of static and
dynamically typed code.

The Takikawa evaluation protocol was originally proposed for Typed
Racket, where static vs dynamic typing is set per module, so $N$ is
the number of modules. Grace, following other languages such as
Reticulated Python \cite{reticPython2014,monotonic2015,Vitousek2017},
allows programmers to choose typechecks at the level of individual
declarations: $N$ is the number of type annotations in the program.,
so checking an entire lattice for a benchmark of any size
is infeasible.  Vitousek et.al.\ modified the Takikawa protocol for
these kinds of languages by taking an approach based on sampling
\cite{vitousek-transient-arXive-2019}.  The Takikawa-Vitousek protocol
divides the number of type annotations in an fully-typed program into
up to 100 intervals, and then randomly generates ten programs within
each interval by erasing type annotations.

To investigate Moth's support for transient typechecks further, we
carried out a (small?) Takikawa-Vitousek evaluation using (some of?)
our existing benchmarks.

\textbf{!!!KJX: Need to update this paragraph with what we did this time around!!!}
To account for the complex warmup behavior
of modern systems\citep{Barrett:2017:VMW} as well as
the non-determinism caused by \eg garbage collection and cache effects,
we run each benchmark for \textbf{$/$NumIterationsAll} iterations in the same
VM invocation.\footnote{
For the Higgs VM, we only use \textbf{$/$NumIterationsHiggs} iterations,
because of its lower performance.
This is sufficient since Higgs's compilation approach induces less variation
and leads to more stable measurements.}
Afterwards, we inspected the run-time plots over the iterations
and manually determined a cutoff of \textbf{$/$WarmupCutOff} iterations for warmup,
\ie, we discard iterations with signs of compilation.
As a result, we use a large number of data points to compute the average,
but outliers, caused by \eg garbage collection, remain visible in the plots.
% In this work, we do not consider startup performance,
% because we want to assess the impact of gradual type checks
% on the best possible performance.
All reported averages use the geometric mean since they aggregate
ratios.


\textbf{
% Yuria
%  - Ubuntu 16.04.4, Kernel 3.13
%  - 24 hyperthreads
%  - Intel Xeon E5-2620 v3 2.40GHz
% Graal 0.33 Feb. 2018
%
% TODO: update these details when we have time... not a rejection criterion,
% but needs to be correct in the final version
All experiments were executed on a machine running Ubuntu Linux 16.04.4,
with Kernel 3.13.
The machine has two Intel Xeon E5-2620 v3 2.40GHz,
with 6 cores each, for a total of 24 hyperthreads.
We used ReBench 0.10.1\citep{ReBench:2018}, Java 1.8.0\_171, Graal 0.33 (\code{a13b888}),
Node.js 10.4, and Higgs from 9 May 2018 (\code{aa95240}).
Benchmarks were executed one by one to avoid interference between them.
The analysis of the results was done with R 3.4.1,
and plots are generated with ggplot 2.2.1 and tikzDevice 0.11.
Our experimental setup is available online to enable reproductions.\footnote{
\url{https://github.com/gracelang/moth-benchmarks}}
}


\section{Results}
\label{s-overall}

give overall results - one page of monochromatic scatterplots one for
each benchmark.

need to explKn how to read scatterplots by paraphrasing
\cite{vitousek-transient-arXive-2019}

core results - i.e.\ looking at one scatter plot per annotation - the original version
   - is there a good benchmark that is small enough to fit on one
   page. and shows interesting results?





\section{Individual Annotations}
\label{s-individual}

We hypothesised that the overhead of an individual type annotation may
be able to predict the performance of program configurations with that
annotation. To test this hypothesis, we constructed a series of
configurations, one for each type annotation, with only that
one annotation included.



   trying-to-be-cleverer results

   - single annotation overheads
  - using them go back to scatterplots (i.e. what you do in the latest graphs) 

  - show that single annotation prediction doesn't really work\ldots

\section{Discussion and Conclusion}
\label{s-concl}

In this paper we have investigated how benchmark results can be
repurposed to determine precisely which transient typechecks are
resistant to the optimisations provided by a just-in-time compiling
virtual machine.  We observed that many of our benchmark results
conducted under the Takikawa protocol 
seemed to have a characteristic bimodal performance profile: some
of the benchmark configurations ran significantly slower than
the remaining configurations. By inspecting the benchmark graphs coded
for each typecheck in a program's source code, we were often able to
identify that just one or two of the typechecks were responsible for
the bimodal performance.  Based on that result, we also investigated
individual benchmark configurations for each individual typecheck:
these results seem to identify problematic typechecks in some but not
all cases. 

This is preliminary work: there are a number of threats to validity. 
Regarding construct validity, 
our underlying
implementation may contain undetected bugs that affect the semantics
or performance of the gradual typing checks. Regarding internal
validity,
our benchmarking harness run on the same implementation is
subject to the same issues. Regarding external validity, 
Moth is built
on the Truffle and Graal toolchain, so we expect
to other Graal
VMs doing similar AST-based optimizations of transient typechecks.
Because we rely on common techniques, 
we expect our results should be transferable to JIT implementations
more widely.


Finally, it is not clear how out results would transfer
to other gradually typed-languages or other semantics for gradual typing.
Our benchmarks do not depend on any features of Grace
that are not common in other gradually-typed object-oriented
languages, but as Grace lacks a large corpus of programs the
benchmarks are necessarily small and artificial.
The advantage of Grace (and Moth) for this research is
that their relative simplicity means we have been able to build an
implementation that features competitive performance with significantly less
effort than would be required for larger and more complex languages.

In the future, we hope to investigate statistical techniques to
determine the significance of each type annotation's contribution to
programs overall performance.  We would also like to investigate 


\bibliography{references}


\end{document}
